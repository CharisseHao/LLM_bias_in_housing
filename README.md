# LLM_bias_in_housing: Investigating LLM Bias in Housing Contexts

This repository focuses on auditing potential biases in responses generated by LLMs in housing related contexts. The notebooks facilitate bulk prompt generation, saving them in `.jsonl` files for submission to several LLMs, and perform analysis of responses across factors such as gender, race, occupation, and family/living status. The goal is to identify and understand potential biases in housing contexts across these variables.


## Getting Started
1. Clone the repository:
    ```
    git clone https://github.com/CharisseHao/LLM_bias_in_housing.git
    ```
2. Navigate to the project directory:
    ```
    cd LLM_bias_in_housing
    ```

To replicate the environment, follow one of the options below:
### Option 1: Recreate the Conda Environment
Ensure that you have Conda installed before proceeding with this option.
1. Create a Conda environment from the `environment.yml` file:
    ```
    conda env create -f environment.yml
    ```
2. Activate the environment:
    ```
    conda activate retail-hiring-bias
    ```

### Option 2: Use `pip` and `requirements.txt` 
Ensure that you have `pip` installed before proceeding with this option.
1. Install the required packages specified in the `requirements.txt` file using pip:
    ```
    pip install -r requirements.txt
    ```

## Descriptions and Instructions:
1. `step1_prompt_bulk_generator.ipynb`
    - Inputs `input_data\audit_names.xlsx` and generates prompts in a `.jsonl` format
        - The files are large, so they are stored as `.zip` files in GitHub. To replicate this process, unzip these files and submit only the raw `.jsonl` file.
2. Prompt Submission Instructions:
    - For OpenAI Models (via Terminal):
        1. Set your API key by replacing `YOUR_SECRET_KEY` with your actual key:
            ```
            batchwizard configure --set-key YOUR_SECRET_KEY
            ```
        2. Navigate to the directory containing the prompt files:
            ```
            cd input_data/batch_requests
            ```
        3. Submit a prompt file, replacing `FILE_NAME` with the actual file name:
            ```
            batchwizard process FILE_NAME.jsonl
            ```
        4. Download the results:
            - Once the request is processed, it will return a `.jsonl` result file. To download it, replace `JOB_ID` with the actual Job ID:
                ```
                batchwizard download JOB_ID
                ```
            - Move the downloaded result file to the `input_data\batch_results` folder.
    - For Other Models (via [Runpod](https://www.runpod.io/console/home)):
        1. Go to [Runpod](https://www.runpod.io/console/home) and select **'Pods'** from the sidebar
        2. Click **'Deploy'**, select a pod, and click **'Edit Template'**:
            - Set **'Volume Disk'** to **500GB**
            - Add two environment variables
                - **Key:** `HF_HOME`, **Value:** `/workspace/.hfhome/`
                - **Key:** `HF_TOKEN`, **Value:** *your Hugging Face API key*
            - Click **'Set Overrides'**, then **'Deploy On-Demand'**.
        3. Once the pod is running, select **'Connect'** and **Jupyter Notebook**
        4. Open a terminal and run:
            ```
            bash step0_run_first.sh
            ```
            This downloads the required packages.
        5. When the process completes, run:
            ```
            bash step2_0_pull_models.sh
            ```
            This downloads the selected models.
        6. Once some models have been downloaded, open a new terminal and run:
            ```
            bash step2_1_run_vllm.sh
            ```
            This submits prompts to the models, and the results are downloaded to the `input_data\batch_results` folder.
